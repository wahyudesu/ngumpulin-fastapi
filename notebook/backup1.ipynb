{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBGcY5MJeuYE",
        "outputId": "faa6b363-bd52-432b-ab5c-da21a47a4d21"
      },
      "outputs": [],
      "source": [
        "!pip -q install supabase torch gliner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "from gliner import GLiNER\n",
        "from supabase import create_client, Client\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, MeanShift, OPTICS\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pickle\n",
        "import optuna\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "model = GLiNER.from_pretrained(\"gliner-community/gliner_medium-v2.5\")\n",
        "\n",
        "model.save_pretrained(\"gliner_Med\")\n",
        "loaded_model = GLiNER.from_pretrained(\"gliner_Med\", load_tokenizer = True, local_files_only=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "Libretto by Marius Petipa, based on the 1822 novella ``Trilby, ou Le Lutin d'Argail`` by Charles Nodier, first presented by the Ballet of the Moscow Imperial Bolshoi Theatre on January 25/February 6 (Julian/Gregorian calendar dates), 1870, in Moscow with Polina Karpakova as Trilby and Ludiia Geiten as Miranda and restaged by Petipa for the Imperial Ballet at the Imperial Bolshoi Kamenny Theatre on January 17–29, 1871 in St. Petersburg with Adèle Grantzow as Trilby and Lev Ivanov as Count Leopold.\n",
        "\"\"\"\n",
        "\n",
        "labels = [\"person\", \"book\", \"location\", \"date\", \"actor\", \"character\"]\n",
        "\n",
        "entities = loaded_model.predict_entities(text, labels, threshold=0.4)\n",
        "\n",
        "for entity in entities:\n",
        "    print(entity[\"text\"], \"=>\", entity[\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "6ZMrdt54d_st",
        "outputId": "9deae7d8-29a4-4d5f-cd47-94b439bec6f6"
      },
      "outputs": [],
      "source": [
        "# Replace with your Supabase project URL and API key\n",
        "url: str = \"https://alwocqtpmrlfebnjjtct.supabase.co\"\n",
        "key: str = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImFsd29jcXRwbXJsZmVibmpqdGN0Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3MzQ0NTAzMDIsImV4cCI6MjA1MDAyNjMwMn0._NZ3uFepvW-JplnMj8jRhbf5CoT4QMS6lB5OJQaxFu4\"\n",
        "\n",
        "supabase: Client = create_client(url, key)\n",
        "\n",
        "# Replace with your table name\n",
        "table_name = \"documents\"\n",
        "\n",
        "# Fetch data from Supabase\n",
        "response = supabase.table(table_name).select(\"*\").execute()\n",
        "\n",
        "df = pd.DataFrame(response.data)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thO7Epoujh7v"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWdsGnwCfTc3"
      },
      "outputs": [],
      "source": [
        "# Assuming 'deadline' and 'uploadedDate' are columns in your DataFrame with datetime values\n",
        "# Convert the columns to datetime objects if they are not already\n",
        "df['deadline'] = pd.to_datetime(df['deadline'])\n",
        "df['uploadedDate'] = pd.to_datetime(df['uploadedDate'])\n",
        "\n",
        "# Calculate the difference in hours\n",
        "df['timing'] = (df['deadline'] - df['uploadedDate']).dt.total_seconds() / 3600\n",
        "df['timing'] = df['timing'].astype(int)\n",
        "# df['timing']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZBtyXzprS41"
      },
      "outputs": [],
      "source": [
        "plagiarism_rule = [40, 50, 60]\n",
        "\n",
        "no_plagiarism = plagiarism_rule[0]\n",
        "maybe_plagiarism = plagiarism_rule[1]\n",
        "plagiarim = plagiarism_rule[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "il8Vm0LdxBUh"
      },
      "outputs": [],
      "source": [
        "df['plagiarism'] = df['plagiarism'].apply(\n",
        "    lambda row: round(max([v for item in row for v in item.values()]) * 100, 2) if row else 0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IlzvObnzyJiE",
        "outputId": "18529b0b-5f4d-441a-c0d1-7bcd750f0c91"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "kvKOPiPUe_xf",
        "outputId": "1aea8eb4-1c76-484b-f958-f723f3ad4b20"
      },
      "outputs": [],
      "source": [
        "data=df[['sentences', 'page', 'timing', 'plagiarism']]\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx1y4_nh31tN",
        "outputId": "cae17165-6f59-41a4-cbd0-29a7c5d28b1e"
      },
      "outputs": [],
      "source": [
        "# Scaling Data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(data)\n",
        "\n",
        "# Evaluasi Model\n",
        "models = {\n",
        "    'KMeans': KMeans(n_clusters=3, random_state=42),\n",
        "    'AgglomerativeClustering': AgglomerativeClustering(n_clusters=3),\n",
        "    'DBSCAN': DBSCAN(eps=1.2, min_samples=5)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    try:\n",
        "        labels = model.fit_predict(X_scaled)\n",
        "        if len(set(labels)) > 1:  # Jika hanya 1 cluster, silhouette_score tidak bisa dihitung\n",
        "            score = silhouette_score(X_scaled, labels)\n",
        "            results[model_name] = score\n",
        "        else:\n",
        "            results[model_name] = \"Invalid (Single Cluster)\"\n",
        "    except Exception as e:\n",
        "        results[model_name] = f\"Error: {str(e)}\"\n",
        "\n",
        "# Menampilkan hasil evaluasi\n",
        "print('Evaluasi Model:')\n",
        "for model_name, score in results.items():\n",
        "    print(f\"{model_name}: {score}\")\n",
        "\n",
        "# Simpan Model Terbaik\n",
        "best_model_name = max(results, key=lambda x: results[x] if isinstance(results[x], float) else -1)\n",
        "\n",
        "if isinstance(results[best_model_name], float):\n",
        "    best_model = models[best_model_name]\n",
        "    best_model.fit(X_scaled)  # Fit model terbaik lagi\n",
        "\n",
        "    with open('best_model.pkl', 'wb') as file:\n",
        "        pickle.dump(best_model, file)\n",
        "\n",
        "    print(f\"\\nModel terbaik '{best_model_name}' berhasil disimpan sebagai 'best_model.pkl'.\")\n",
        "else:\n",
        "    print(\"Tidak ada model yang valid untuk disimpan.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0ns7i0Y4oHG",
        "outputId": "11bab43b-2dfe-4c24-9a3b-a10776d876d0"
      },
      "outputs": [],
      "source": [
        "# Scaling Data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(data)\n",
        "\n",
        "# Definisi Model\n",
        "models = {\n",
        "    'KMeans': KMeans(),\n",
        "    'AgglomerativeClustering': AgglomerativeClustering(),\n",
        "    'DBSCAN': DBSCAN(),\n",
        "    'MeanShift': MeanShift(),\n",
        "    'OPTICS': OPTICS()\n",
        "}\n",
        "\n",
        "# Hyperparameter Tuning\n",
        "params = {\n",
        "    'KMeans': {'n_clusters': [2, 3, 4, 5]},\n",
        "    'AgglomerativeClustering': {'n_clusters': [2, 3, 4, 5]},\n",
        "    'DBSCAN': {'eps': [0.5, 1.0, 1.5], 'min_samples': [5, 10]},\n",
        "    'MeanShift': {},  # Tidak perlu tuning\n",
        "    'OPTICS': {'min_samples': [5, 10], 'xi': [0.05, 0.1, 0.2]}\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    if params[model_name]:  # Jika ada hyperparameter yang perlu di-tuning\n",
        "        grid_search = GridSearchCV(model, params[model_name], scoring='adjusted_rand_score', cv=3, n_jobs=-1)\n",
        "        grid_search.fit(X_scaled, np.zeros(X_scaled.shape[0]))\n",
        "        best_model = grid_search.best_estimator_\n",
        "    else:  # Untuk model tanpa hyperparameter\n",
        "        best_model = model\n",
        "        best_model.fit(X_scaled)\n",
        "\n",
        "    labels = best_model.fit_predict(X_scaled)\n",
        "    if len(set(labels)) > 1:  # Jika lebih dari 1 cluster\n",
        "        score = silhouette_score(X_scaled, labels)\n",
        "        results[model_name] = (score, best_model)\n",
        "    else:\n",
        "        results[model_name] = ('Invalid (Single Cluster)', None)\n",
        "\n",
        "# Menampilkan hasil evaluasi\n",
        "print('Evaluasi Model:')\n",
        "for model_name, (score, _) in results.items():\n",
        "    print(f\"{model_name}: {score}\")\n",
        "\n",
        "# Menyimpan model terbaik\n",
        "best_model_name = max(results, key=lambda x: results[x][0] if isinstance(results[x][0], float) else -1)\n",
        "best_model = results[best_model_name][1]\n",
        "\n",
        "if best_model:\n",
        "    with open('best_model.pkl', 'wb') as file:\n",
        "        pickle.dump(best_model, file)\n",
        "    print(f\"\\nModel terbaik '{best_model_name}' berhasil disimpan sebagai 'best_model.pkl'.\")\n",
        "else:\n",
        "    print(\"Tidak ada model yang valid untuk disimpan.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4JThfbit_8y",
        "outputId": "678e00b2-a47b-4ce2-b8ff-14e405ab7a3c"
      },
      "outputs": [],
      "source": [
        "# prompt: buatkan saya kode untuk membaca pickle dan melakukan prediksi berdasarkan data baru\n",
        "\n",
        "# Load the saved model\n",
        "with open('best_model.pkl', 'rb') as file:\n",
        "    loaded_model = pickle.load(file)\n",
        "\n",
        "# New data for prediction\n",
        "new_data = pd.DataFrame({\n",
        "    'sentences': [100],  # Replace with your actual data\n",
        "    'page': [5],  # Replace with your actual data\n",
        "    'timing': [48],  # Replace with your actual data\n",
        "    'plagiarism': [80.00]  # Replace with your actual data\n",
        "})\n",
        "\n",
        "# Preprocess the new data\n",
        "scaler = StandardScaler()  # Assuming you used StandardScaler for training\n",
        "X_new_scaled = scaler.fit_transform(new_data)\n",
        "\n",
        "# Make predictions\n",
        "prediction = loaded_model.fit_predict(X_new_scaled)\n",
        "\n",
        "print(\"Prediction:\", prediction)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
